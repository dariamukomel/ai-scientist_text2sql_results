# AI-Scientist Text-to-SQL — Result Bundles

This repository contains **complete result bundles** for three successful ideas that improved a Text-to-SQL benchmark using the AI-Scientist workflow.

- Two ideas were executed end-to-end with **DeepSeek** (baseline, idea generation, experiments, paper drafting).
- One idea was executed with **GigaChat-2-Max** for the baseline + full pipeline, while **DeepSeek-Reasoner** generated the strategies.

---

## Results at a Glance

### DeepSeek pipeline (baseline + ideas + experiments + paper with DeepSeek)
- **20250722_004521_lightweight_metadata_feedback** → **Total accuracy: 93.3%** (best at **run 29**).  
  DeepSeek baseline for this track: **91.7%**.

- **20250721_182436_relationship_aware_prompting** → **Total accuracy: 95.0%** (best at **run 8**).  
  DeepSeek baseline for this track: **91.7%**.

### GigaChat pipeline (DeepSeek-Reasoner for ideas; baseline + experiments + paper with GigaChat-2-Max)
- **20250721_182436_relationship_aware_prompting** (GigaChat execution) → **86.7% → 91.7%** (best at **run 25**).

> Note: You will find separate folders for these runs; the folder contents indicate which engine was used.

---

## Model Versions

The following model identifiers were used:

- **deepseek-chat** → `DeepSeek-V3-0324`  
- **deepseek-reasoner** → `DeepSeek-R1-0528`  
- **deepseek-coder** → `DeepSeek-Coder-V2`  
- **GigaChat** execution track → `gigachat-2-max`

---

## What’s in Each Idea Folder

Each idea directory includes all artifacts needed to inspect and reproduce the results. Filenames are **concrete files** (not subfolders) unless otherwise noted, and exact names may vary by idea/timestamp.

- `experiment.py` — **final best run** implementation for the idea.  
- `baseline.py` — the baseline implementation used for this track.  
- `prompt.json` — the prompt configuration provided to AI‑Scientist.  
- `ideas.json` — all candidate strategies generated during that launch.  
- `seed_ideas.json` — seed ideas used to bootstrap strategy generation.  
- `plot.py` — script (auto‑generated by AI‑Scientist) to produce the plots.  
- **Per‑run code, metrics, and SQL** — all runs are stored with their code, `final_info.json` metrics, and generated SQL outputs.  
- `notes.txt` — **human‑readable summary** of what the LLM changed at **each run** (short per‑run notes).  
- **Paper & logs** — the generated paper and Aider logs are present as files with idea‑specific names, e.g.:
  - `*.pdf` — the manuscript produced for this idea.  
  - `*_aider.txt`— logs from the Aider tool.  

**Only for DeepSeek idea folders:**
- `review.txt` — **review logs** from the autonomous paper review stage.  
- `*_improved.pdf` — the **reviewed** (revised) manuscript produced after the review cycle.

---

## How to Read the Metrics

- **Total accuracy** — execution‑based acceptance across **all** difficulty buckets.  
- **Easy+Medium accuracy** — execution‑based acceptance restricted to **easy + medium** queries.  
- **Gold alignment statistics** — for each difficulty bucket (easy, medium, hard), a distribution of predicted SQL vs. gold SQL by percent‑overlap intervals: `0%`, `(0–25%]`, `(25–50%]`, `(50–75%]`, `(75–100%]`.

Plotting scripts (`plot.py`) are included per idea to reproduce the charts found in the papers.



